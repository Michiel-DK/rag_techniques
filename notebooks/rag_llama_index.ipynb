{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'Meta-Llama-3-8B-Instruct.Q4_0.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /Users/michieldekoninck/Library/Caches/llama_index/models/Meta-Llama-3-8B-Instruct.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = dl\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
      "llm_load_print_meta: general.name     = dl\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4156.02 MiB, ( 7168.91 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4156.00 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 7008\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   876.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  876.00 MiB, K (f16):  438.00 MiB, V (f16):  438.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   483.69 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    21.69 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'dl', 'llama.vocab_size': '128256'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=7000,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6548.08 ms\n",
      "llama_print_timings:      sample time =      25.62 ms /   256 runs   (    0.10 ms per token,  9990.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6547.95 ms /    76 tokens (   86.16 ms per token,    11.61 tokens per second)\n",
      "llama_print_timings:        eval time =    8269.06 ms /   255 runs   (   32.43 ms per token,    30.84 tokens per second)\n",
      "llama_print_timings:       total time =   15134.13 ms /   331 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <<SYS>>  [INST]  You can find many poems about cats and dogs online. Here is one example: \"Cats and dogs, they're quite a pair, / With fur so soft and eyes so fair. / They chase and play and snuggle tight, / And bring us joy with their antics bright. / So here's to our feline and canine friends, / Who bring us laughter that never ends.\"  [INST]  <<SYS>>  [INST]  Would you like me to find another one?  [/INST]  <<SYS>>  [INST]  <<SYS>>  [/INST]  <<SYS>>  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import BaseNode, TransformComponent\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "from langchain_community.llms import Ollama\n",
    "import faiss\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMENSION = 512\n",
    "\n",
    "# llamaindex ==> length of the tokens\n",
    "CHUNK_SIZE = 200\n",
    "CHUNK_OVERLAP = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/\"\n",
    "node_parser = SimpleDirectoryReader(input_dir=path, required_exts=['.pdf'])\n",
    "documents = node_parser.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FaisVectorStore to store embeddings\n",
    "faiss_index = faiss.IndexFlatL2(EMBED_DIMENSION)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner(TransformComponent):\n",
    "    \"\"\"\n",
    "    Transformation to be used within the ingestion pipeline.\n",
    "    Cleans clutters from texts.\n",
    "    \"\"\"\n",
    "    def __call__(self, nodes, **kwargs) -> List[BaseNode]:\n",
    "        \n",
    "        for node in nodes:\n",
    "            node.text = node.text.replace('\\t', ' ') # Replace tabs with spaces\n",
    "            node.text = node.text.replace(' \\n', ' ') # Replace paragraph seperator with spacaes\n",
    "            \n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "# Create a pipeline with defined document transformations and vectorstore\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        TextCleaner(),\n",
    "        text_splitter,\n",
    "    ],\n",
    "    vector_store=vector_store, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline and get generated nodes from the process\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store_index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)\n",
    "retriever = vector_store_index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 59 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6548.08 ms\n",
      "llama_print_timings:      sample time =      34.64 ms /   256 runs   (    0.14 ms per token,  7390.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4072.18 ms /   448 tokens (    9.09 ms per token,   110.01 tokens per second)\n",
      "llama_print_timings:        eval time =    8863.60 ms /   255 runs   (   34.76 ms per token,    28.77 tokens per second)\n",
      "llama_print_timings:       total time =   13318.69 ms /   703 tokens\n"
     ]
    }
   ],
   "source": [
    "Settings.llm = llm\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "#define evaluator\n",
    "evaluator = FaithfulnessEvaluator(llm=Settings.llm)\n",
    "\n",
    "# query index\n",
    "query_engine = vector_store_index.as_query_engine(llm = Settings.llm)\n",
    "response = query_engine.query(\n",
    "    \"What is the main cause of climate change?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 59 prefix-match hit, remaining 823 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6548.08 ms\n",
      "llama_print_timings:      sample time =      44.44 ms /   256 runs   (    0.17 ms per token,  5761.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3259.72 ms /   823 tokens (    3.96 ms per token,   252.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9168.94 ms /   255 runs   (   35.96 ms per token,    27.81 tokens per second)\n",
      "llama_print_timings:       total time =   12977.78 ms /  1078 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "print(str(eval_result.passing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='  <<SYS>>  The main cause of climate change is the increase in greenhouse gases in the atmosphere.  <</SYS>>  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/INST]  [/', source_nodes=[NodeWithScore(node=TextNode(id_='1d6c03eb-f9cc-4258-af67-6f2e4f6e19ac', embedding=None, metadata={'page_label': '1', 'file_name': 'Understanding_Climate_Change.pdf', 'file_path': '/Users/michieldekoninck/code/Michiel-DK/rag_techniques/notebooks/../data/Understanding_Climate_Change.pdf', 'file_type': 'application/pdf', 'file_size': 206372, 'creation_date': '2024-09-15', 'last_modified_date': '2024-09-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8f157564-9983-47b5-9df3-ff9a00c46a3a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'Understanding_Climate_Change.pdf', 'file_path': '/Users/michieldekoninck/code/Michiel-DK/rag_techniques/notebooks/../data/Understanding_Climate_Change.pdf', 'file_type': 'application/pdf', 'file_size': 206372, 'creation_date': '2024-09-15', 'last_modified_date': '2024-09-15'}, hash='425ad763591ff573839b77603bd5145670992a4fd9ad661ea984a2d5f2edbd9c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2490d9b9-b99b-46f3-b488-5839c48955f7', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'Understanding_Climate_Change.pdf', 'file_path': '/Users/michieldekoninck/code/Michiel-DK/rag_techniques/notebooks/../data/Understanding_Climate_Change.pdf', 'file_type': 'application/pdf', 'file_size': 206372, 'creation_date': '2024-09-15', 'last_modified_date': '2024-09-15'}, hash='e1c3bbd26f3e613d7a5e51617544a601383a146cbc9a92946f3616fe329d75de'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d76eca54-a724-4d7a-8ca9-04a060bebb1d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6c1df89861a900fbb7515b8fc765ac483f58cbb6c01ede7f0039c9e5de37be7b')}, text='The evidence overwhelmingly shows that recent changes are primarily driven by human activities, particularly the emission of greenhou se gases.  Chapter 2: Causes of Climate Change  Greenhouse Gases  The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is  essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.  Fossil Fuels  Burning fossil fuels for energy releases large amounts of CO2.', mimetype='text/plain', start_char_idx=1466, end_char_idx=2176, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8150816466189671), NodeWithScore(node=TextNode(id_='457e8f1e-4499-47a7-b530-3e968e7545d1', embedding=None, metadata={'page_label': '1', 'file_name': 'Understanding_Climate_Change.pdf', 'file_path': '/Users/michieldekoninck/code/Michiel-DK/rag_techniques/notebooks/../data/Understanding_Climate_Change.pdf', 'file_type': 'application/pdf', 'file_size': 206372, 'creation_date': '2024-09-15', 'last_modified_date': '2024-09-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8f157564-9983-47b5-9df3-ff9a00c46a3a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'Understanding_Climate_Change.pdf', 'file_path': '/Users/michieldekoninck/code/Michiel-DK/rag_techniques/notebooks/../data/Understanding_Climate_Change.pdf', 'file_type': 'application/pdf', 'file_size': 206372, 'creation_date': '2024-09-15', 'last_modified_date': '2024-09-15'}, hash='425ad763591ff573839b77603bd5145670992a4fd9ad661ea984a2d5f2edbd9c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2490d9b9-b99b-46f3-b488-5839c48955f7', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='427274e9108ece002884c4500c34624131f01c0c25b4b1a748c5c9bc9117ac96')}, text='Understanding Climate Change  Chapter 1: Introduction to Climate Change  Climate change refers to significant, long -term changes in the global climate. The term \"global climate\" encompasses the planet\\'s overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. Over the past cent ury, human activities, particularly the burning of fossil fuels and deforestation, have significantly contributed to climate change.  Historical Context  The Earth\\'s climate has changed throughout history. Over the past 650,000 years, there have been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about 11,700 years ago marking the beginning of the modern climate era and  human civilization.', mimetype='text/plain', start_char_idx=0, end_char_idx=762, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7690607270990159)], metadata={'1d6c03eb-f9cc-4258-af67-6f2e4f6e19ac': {'page_label': '1', 'file_name': 'Understanding_Climate_Change.pdf', 'file_path': '/Users/michieldekoninck/code/Michiel-DK/rag_techniques/notebooks/../data/Understanding_Climate_Change.pdf', 'file_type': 'application/pdf', 'file_size': 206372, 'creation_date': '2024-09-15', 'last_modified_date': '2024-09-15'}, '457e8f1e-4499-47a7-b530-3e968e7545d1': {'page_label': '1', 'file_name': 'Understanding_Climate_Change.pdf', 'file_path': '/Users/michieldekoninck/code/Michiel-DK/rag_techniques/notebooks/../data/Understanding_Climate_Change.pdf', 'file_type': 'application/pdf', 'file_size': 206372, 'creation_date': '2024-09-15', 'last_modified_date': '2024-09-15'}})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
